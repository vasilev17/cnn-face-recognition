{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5576adf9-c454-4b1c-b653-fc21d4707fce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding shape: torch.Size([1, 512])\n",
      "Exiting... (Window closed)\n"
     ]
    }
   ],
   "source": [
    "#Import the needed libraries\n",
    "import cv2\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import torch\n",
    "import pygetwindow as gw\n",
    "\n",
    "# Initialize MTCNN for face detection and InceptionResnetV1 for face recognition\n",
    "mtcnn = MTCNN(keep_all=True)\n",
    "model = InceptionResnetV1(pretrained='vggface2').eval()  # Load the pre-trained face recognition model\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(r'D:\\Coding Projects\\CNN (face recog)\\two_face_recog_test_video.mp4')\n",
    "\n",
    "# Set the window size for display\n",
    "window_width = 800\n",
    "window_height = 600\n",
    "\n",
    "# Set the position of the window (x, y)\n",
    "window_x_position = 100\n",
    "window_y_position = 100\n",
    "\n",
    "# Create a named window and set the size\n",
    "cv2.namedWindow('Video', cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow('Video', window_width, window_height)\n",
    "\n",
    "# Move the window to the desired position\n",
    "cv2.moveWindow('Video', window_x_position, window_y_position)\n",
    "\n",
    "# Set the window always on top using pygetwindow (Windows-specific)\n",
    "window = gw.getWindowsWithTitle('Video')[0]\n",
    "window.alwaysOnTop = True\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to RGB (the model expects RGB)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Detect faces in the frame using MTCNN\n",
    "    boxes, probs = mtcnn.detect(rgb_frame)\n",
    "\n",
    "    # If no faces are detected, continue to the next frame\n",
    "    if boxes is None:\n",
    "        cv2.imshow('Video', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "        continue\n",
    "\n",
    "    # Process detected faces\n",
    "    for i, box in enumerate(boxes):\n",
    "        # Draw a rectangle around the detected face\n",
    "        cv2.rectangle(frame, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (255, 0, 0), 4)\n",
    "\n",
    "        # Ensure the face is large enough before passing it to the model\n",
    "        face_width = int(box[2]) - int(box[0])\n",
    "        face_height = int(box[3]) - int(box[1])\n",
    "\n",
    "        if face_width < 50 or face_height < 50:\n",
    "            # If the face is too small, skip processing this face\n",
    "            continue\n",
    "\n",
    "        # Extract the face from the frame (make sure the region is valid)\n",
    "        face = rgb_frame[int(box[1]):int(box[3]), int(box[0]):int(box[2])]\n",
    "\n",
    "        # Ensure the face region is not empty\n",
    "        if face.size == 0:\n",
    "            continue  # Skip invalid or empty face regions\n",
    "\n",
    "        # Resize face to 160x160, as expected by InceptionResnetV1\n",
    "        face_resized = cv2.resize(face, (160, 160))\n",
    "\n",
    "        # Convert face to tensor and normalize\n",
    "        face_tensor = torch.from_numpy(face_resized).permute(2, 0, 1).float()  # Convert to tensor (C, H, W)\n",
    "        face_tensor = face_tensor.unsqueeze(0)  # Add batch dimension\n",
    "        face_tensor = face_tensor / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "        # Get the face embedding (feature vector)\n",
    "        embedding = model(face_tensor)\n",
    "\n",
    "        # For demonstration, we just print the embedding shape\n",
    "        print(f'Embedding shape: {embedding.shape}')\n",
    "\n",
    "    # Show the frame with face detections\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Check for exit conditions\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # Exit on 'q' or 'ESC' key press\n",
    "    if key == ord('q') or key == 27:  # 27 is the ASCII code for Escape key\n",
    "        print(\"Exiting... (Escape or 'q' pressed)\")\n",
    "        break\n",
    "\n",
    "    # If the video window is closed, stop the execution\n",
    "    if cv2.getWindowProperty(\"Video\", cv2.WND_PROP_VISIBLE) < 1:\n",
    "        print(\"Exiting... (Window closed)\")\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c220d1de-34a0-4db1-bf60-b7a6141048a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
